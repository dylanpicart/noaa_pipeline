{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "078fa8eb",
   "metadata": {},
   "source": [
    "# Data Engineering Project Roadmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "base_url = \"https://coastwatch.pfeg.noaa.gov/erddap/griddap/erdMWcflhmday.json\"\n",
    "params = {\n",
    "    \"latitude\": \"32<=latitude<=49\",\n",
    "    \"longitude\": \"235<=longitude<=243\",\n",
    "    \"time\": \"2023-01-01<=time<=2023-01-02\"\n",
    "}\n",
    "\n",
    "query_string = urllib.parse.urlencode(params, safe=\"<=,\")\n",
    "url = f\"{base_url}?time,latitude,longitude,flh&{query_string}\"\n",
    "\n",
    "print(f\"Query URL: {url}\")\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n",
    "else:\n",
    "    print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a50d7",
   "metadata": {},
   "source": [
    "### Project Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "ðŸ“ noaa_pipeline/\n",
    "â”‚\n",
    "â”œâ”€â”€ ðŸ“‚ .docker/\n",
    "â”‚   â”œâ”€â”€ ðŸ“„ Dockerfile.airflow\n",
    "â”‚   â”œâ”€â”€ ðŸ“„ Dockerfile.graphql\n",
    "â”‚   â””â”€â”€ ðŸ“„ Dockerfile.spark\n",
    "â”‚\n",
    "â”œâ”€â”€ ðŸ“‚ dags/\n",
    "â”‚   â”œâ”€â”€ ðŸ“„ noaa_etl_dag.py\n",
    "â”‚   â”œâ”€â”€ ðŸ“„ __init__.py\n",
    "â”‚   â””â”€â”€ ðŸ“‚ scripts/\n",
    "â”‚       â”œâ”€â”€ ðŸ“„ extract_noaa_data.py\n",
    "â”‚       â”œâ”€â”€ ðŸ“„ transform_data.py\n",
    "â”‚       â””â”€â”€ ðŸ“„ upload_to_gcp.py\n",
    "â”‚\n",
    "â”œâ”€â”€ ðŸ“‚ graphql/                       # Renamed from graphql_server\n",
    "â”‚   â”œâ”€â”€ ðŸ“‚ resolvers/\n",
    "â”‚   â”‚   â”œâ”€â”€ ðŸ“„ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ ðŸ“„ pmn_resolver.py\n",
    "â”‚   â”‚   â”œâ”€â”€ ðŸ“„ buoy_resolver.py\n",
    "â”‚   â”‚   â””â”€â”€ ðŸ“„ climate_resolver.py\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ ðŸ“‚ spark_utils/\n",
    "â”‚   â”‚   â”œâ”€â”€ ðŸ“„ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ ðŸ“„ spark_session.py\n",
    "â”‚   â”‚   â””â”€â”€ ðŸ“„ hdfs_helpers.py\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ ðŸ“‚ utils/\n",
    "â”‚   â”‚   â”œâ”€â”€ ðŸ“„ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ ðŸ“„ api_requests.py\n",
    "â”‚   â”‚   â””â”€â”€ ðŸ“„ logging_setup.py\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ ðŸ“„ schema.py\n",
    "â”‚   â”œâ”€â”€ ðŸ“„ app.py\n",
    "â”‚   â””â”€â”€ ðŸ“„ requirements.txt\n",
    "â”‚\n",
    "â”œâ”€â”€ ðŸ“‚ spark_jobs/\n",
    "â”‚   â”œâ”€â”€ ðŸ“„ transform_pmn.py\n",
    "â”‚   â”œâ”€â”€ ðŸ“„ transform_buoy.py\n",
    "â”‚   â””â”€â”€ ðŸ“„ transform_climate.py\n",
    "â”‚\n",
    "â”œâ”€â”€ ðŸ“‚ data/\n",
    "â”‚   â”œâ”€â”€ ðŸ“‚ raw/\n",
    "â”‚   â”œâ”€â”€ ðŸ“‚ transformed/\n",
    "â”‚   â”œâ”€â”€ ðŸ“‚ postgres_data/\n",
    "â”‚   â””â”€â”€ ðŸ“‚ hadoop_data/\n",
    "â”‚\n",
    "â”œâ”€â”€ ðŸ“„ docker-compose.yaml\n",
    "â”œâ”€â”€ ðŸ“„ .dockerignore\n",
    "â”œâ”€â”€ ðŸ“„ .env\n",
    "â”œâ”€â”€ ðŸ“„ requirements.txt\n",
    "â”œâ”€â”€ ðŸ“„ .gitignore\n",
    "â””â”€â”€ ðŸ“„ README.md\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad8587",
   "metadata": {},
   "source": [
    "## Phase 1: Project Setup and Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a82669",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Set up the tools, environment, and infrastructure needed to run your project.\n",
    "\n",
    "1. **Set Up AWS Resources:**\n",
    "   - **S3 Buckets**:\n",
    "     - Create two S3 buckets: \n",
    "       - **Raw Data Storage** (e.g., `s3://your-bucket/raw-data/`)\n",
    "       - **Processed Data Storage** (e.g., `s3://your-bucket/processed-data/`)\n",
    "   - **IAM Roles and Policies**:\n",
    "     - Create an **IAM Role** with:\n",
    "       - **S3 read/write access**\n",
    "       - **Glue and Redshift permissions**\n",
    "     - Attach the role to your **Glue jobs** and **Redshift** COPY operations.\n",
    "\n",
    "2. **Set Up PostgreSQL (Optional for Metadata Storage)**:\n",
    "   - Install PostgreSQL or use **Amazon RDS** to store metadata or staging data.\n",
    "\n",
    "3. **Install and Configure Airflow**:\n",
    "   - Use **Docker** or **Local Setup** to install Airflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apache-airflow boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbcb358",
   "metadata": {},
   "source": [
    "\n",
    "4. **Install Required Packages**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d52b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68db752b",
   "metadata": {},
   "source": [
    "## Phase 2: Data Extraction and Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456e87b",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Ingest data into S3 and set up the initial pipeline to load data into PostgreSQL and Redshift.\n",
    "\n",
    "Below is a Python code example for extracting data from S3 using **Boto3**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "\n",
    "def extract_data_from_s3():\n",
    "    # Download data from S3 to local storage.\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file('noaa-nexrad-level2', '2022/01/01/sample.csv', './data/sample.csv')\n",
    "    print(\"Data downloaded from S3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5263b",
   "metadata": {},
   "source": [
    "## Phase 3: AWS Glue Development and ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11388dc6",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Set up AWS Glue for data transformation and write PySpark code for ETL tasks.\n",
    "\n",
    "Below is a PySpark code example for temperature conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5107d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NOAA_ETL\").getOrCreate()\n",
    "df = spark.read.csv(\"s3://your-bucket/raw-data/sample.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_transformed = df.withColumn(\n",
    "    'temp_celsius', (df['temperature'] - 32) * 5.0 / 9.0\n",
    ")\n",
    "df_transformed.write.parquet(\"s3://your-bucket/processed-data/\")\n",
    "print(\"Data transformed and written to S3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56265bfe",
   "metadata": {},
   "source": [
    "## Phase 4: Data Transformation and Redshift Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e69e38",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Write transformed data to Redshift and create analytical tables for Tableau.\n",
    "\n",
    "Below is the SQL COPY command for loading data from S3 to Redshift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COPY noaa_weather\n",
    "FROM 's3://your-bucket/processed-data/'\n",
    "IAM_ROLE 'arn:aws:iam::your-account-id:role/RedshiftCopyRole'\n",
    "FORMAT AS PARQUET;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e0bef",
   "metadata": {},
   "source": [
    "## Phase 5: Data Validation and Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6945259",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Automate data validation and error handling for your pipeline.\n",
    "\n",
    "Example Python code for data validation using Airflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def validate_data():\n",
    "    df = pd.read_csv('./data/sample.csv')\n",
    "    missing_count = df['temperature'].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        raise ValueError(f\"{missing_count} missing values found!\")\n",
    "    print(\"Data validation passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b11a4",
   "metadata": {},
   "source": [
    "## Phase 6: LLM Integration for Documentation and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a8385",
   "metadata": {},
   "source": [
    "\n",
    "Below is a Python function using OpenAI API to generate data documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0fea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "\n",
    "def generate_documentation():\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        prompt=\"Generate documentation for the NOAA dataset...\",\n",
    "        max_tokens=200\n",
    "    )\n",
    "    print(response.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4070e26",
   "metadata": {},
   "source": [
    "## Phase 7: Tableau Integration and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20779089",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Visualize the transformed data using Tableau dashboards.\n",
    "\n",
    "1. **Export Transformed Data**:\n",
    "   - Use the following code to export data from Redshift to CSV for Tableau.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e657680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def export_data_to_csv():\n",
    "    df = pd.read_sql(\"SELECT * FROM noaa_weather\", con=your_redshift_connection)\n",
    "    df.to_csv(\"noaa_weather.csv\", index=False)\n",
    "    print(\"Data exported to CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cd102",
   "metadata": {},
   "source": [
    "## Phase 8: Deployment and Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a22fe",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Deploy the entire pipeline and set up automation for production.\n",
    "\n",
    "Example Airflow DAG snippet for automating the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ae1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG('noaa_pipeline', start_date=datetime(2024, 10, 17), schedule_interval='@daily') as dag:\n",
    "    extract_task = PythonOperator(task_id='extract_data', python_callable=extract_data_from_s3)\n",
    "    validate_task = PythonOperator(task_id='validate_data', python_callable=validate_data)\n",
    "    extract_task >> validate_task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af75a2f",
   "metadata": {},
   "source": [
    "## Phase 9: Testing, Optimization, and Future Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021790f8",
   "metadata": {},
   "source": [
    "\n",
    "### Goal: Test the entire pipeline and plan for future extensions.\n",
    "\n",
    "1. **Test End-to-End Execution**: Verify that each step works as expected.\n",
    "2. **Optimize Performance**: Partition datasets and tune queries.\n",
    "3. **Explore Databricks**: Consider Databricks for future ML-based projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b896bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c9cd480",
   "metadata": {},
   "source": [
    "1. NOAA PMN (Phytoplankton Monitoring Network) Dataset\n",
    "API Access (ERDDAP Server):\n",
    "PMN Dataset (in JSON):\n",
    "https://coastwatch.pfeg.noaa.gov/erddap/tabledap/noaa_pmn.json\n",
    "\n",
    "PMN Dataset Metadata and Parameter Documentation:\n",
    "https://coastwatch.pfeg.noaa.gov/erddap/tabledap/noaa_pmn.html\n",
    "\n",
    "Description of Key Parameters:\n",
    "\n",
    "time: The timestamp of the observation.\n",
    "\n",
    "latitude / longitude: Geographic location of the sample.\n",
    "\n",
    "chlorophyll: Chlorophyll concentration in the water (mg/mÂ³).\n",
    "\n",
    "temperature: Sea surface temperature at the time of observation (Â°C).\n",
    "\n",
    "salinity: Salinity levels (PSU - Practical Salinity Units).\n",
    "\n",
    "2. NOAA Buoy Data (NDBC)\n",
    "\n",
    "API Access:\n",
    "\n",
    "Station Data (JSON):\n",
    "https://www.ndbc.noaa.gov/data/realtime2/\n",
    "\n",
    "Example: https://www.ndbc.noaa.gov/data/realtime2/41009.txt\n",
    "\n",
    "Buoy Metadata and Documentation:\n",
    "https://www.ndbc.noaa.gov/measdes.shtml\n",
    "\n",
    "Description of Key Parameters:\n",
    "\n",
    "YY / MM / DD / hh / mm: Year, month, day, hour, and minute of observation.\n",
    "\n",
    "WDIR: Wind direction (Â° from true north).\n",
    "\n",
    "WSPD: Wind speed (m/s).\n",
    "\n",
    "GST: Wind gust (m/s).\n",
    "\n",
    "WVHT: Wave height (meters).\n",
    "\n",
    "DPD: Dominant wave period (seconds).\n",
    "\n",
    "APD: Average wave period (seconds).\n",
    "\n",
    "PRES: Atmospheric pressure at sea level (hPa).\n",
    "\n",
    "ATMP: Air temperature (Â°C).\n",
    "\n",
    "WTMP: Sea surface temperature (Â°C).\n",
    "\n",
    "3. NOAA Climate Data (NCDC)\n",
    "\n",
    "API Access:\n",
    "\n",
    "NOAA Climate Data Online (CDO) API:\n",
    "https://www.ncdc.noaa.gov/cdo-web/webservices/v2\n",
    "\n",
    "API Token Registration:\n",
    "To access the NOAA CDO API, youâ€™ll need to register for a token here:\n",
    "https://www.ncdc.noaa.gov/cdo-web/token\n",
    "\n",
    "Endpoints:\n",
    "\n",
    "Datasets: https://www.ncdc.noaa.gov/cdo-web/api/v2/datasets\n",
    "Locations: https://www.ncdc.noaa.gov/cdo-web/api/v2/locations\n",
    "\n",
    "Description of Key Parameters:\n",
    "DATE: Date of observation.\n",
    "\n",
    "TMAX / TMIN: Maximum and minimum temperatures (Â°C).\n",
    "\n",
    "PRCP: Precipitation (mm).\n",
    "\n",
    "SNOW: Snowfall (mm).\n",
    "\n",
    "WIND: Wind speed (m/s).\n",
    "\n",
    "EVAP: Evaporation (mm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348003c1",
   "metadata": {},
   "source": [
    "It may be useful to leverage ArcGIS in order to get a good visual analysis of the region we are running our analysis on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
